# 语音增强
*** 
## 预备知识
### 因果卷积
>参考知乎回答：[因果卷积神经网络 —— 专为时间序列预测而设计的深度学习网络结构](https://zhuanlan.zhihu.com/p/422177151)  
>代码参考博客园：[TCN与因果卷积](https://www.cnblogs.com/PythonLearner/p/12925732.html)

output通过卷积考虑此前时刻的输入
![](img/mk-2023-09-09-11-00-06.png)
使用扩张卷积扩大感受野
![](img/mk-2023-09-09-11-01-00.png)
实现代码：D:\Code\KWS\SpeechEnhancement\CausalConvNet

### ELU（指数线性单元,激活函数的一种）
ELU 的提出也解决了ReLU 的问题。与ReLU相比，ELU有负值，这会使激活的平均值接近零。均值激活接近于零可以使学习更快，因为它们使梯度更接近自然梯度。
![](img/mk-2023-09-10-11-37-03.png)  
ELU函数的特点:  
- 没有Dead ReLU问题，输出的平均值接近0，以0为中心。
- ELU 通过减少偏置偏移的影响，使正常梯度更接近于单位自然梯度，从而使均值向零加速学习。
- ELU函数在较小的输入下会饱和至负值，从而减少前向传播的变异和信息。
- ELU函数的计算强度更高。与Leaky ReLU类似，尽管理论上比ReLU要好，但目前在实践中没有充分的证据表明ELU总是比ReLU好。
![](img/mk-2023-09-10-11-38-55.png)
### STFT幅度谱
![](img/mk-2023-09-10-15-14-53.png)
***
## 用于语音增强的CRN（卷积循环网络）
>来自Tan K, Wang D L. A convolutional recurrent neural network for real-time speech enhancement[C]//Interspeech. 2018, 2018: 3229-3233.



***
## DCCRN
>内容来自Hu Y, Liu Y, Lv S, et al. DCCRN: Deep complex convolution recurrent network for phase-aware speech enhancement[J]. arXiv preprint arXiv:2008.00264, 2020.

